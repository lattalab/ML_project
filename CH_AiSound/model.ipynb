{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基本設置 + 設cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Load in relevant libraries, and alias where appropriate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define relevant variables for the ML task\n",
    "batch_size = 64\n",
    "num_classes = 2\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n",
    "\n",
    "# Device will determine whether to run the training on GPU or CPU.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分割資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fake', 'real']\n",
      "{'fake': 0, 'real': 1}\n",
      "778\n"
     ]
    }
   ],
   "source": [
    "import torchvision.datasets as dset\n",
    "\n",
    "# 定义数据转换\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 480)),  # 将图像调整为指定大小\n",
    "    transforms.ToTensor(),           # 将图像转换为张量\n",
    "])\n",
    "\n",
    "# 加載圖像數據集，並只考慮指定的資料夾\n",
    "dataset = dset.ImageFolder(root='D:/Project/CH_AiSound', transform=transform)\n",
    "\n",
    "# 獲取相關信息\n",
    "print(dataset.classes)  # 根據分的文件夾的名字來確定類別\n",
    "print(dataset.class_to_idx)  # 按順序為這些類別定義索引為 0, 1, ...\n",
    "print(len(dataset))  # 返回數據集的大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, SubsetRandomSampler, ConcatDataset\n",
    "\n",
    "# 划分数据集\n",
    "# 首先确定数据集的大小\n",
    "dataset_size = len(dataset)\n",
    "# 然后确定划分比例\n",
    "valid_split = 0.25\n",
    "# 计算测试集的大小\n",
    "valid_size = int(valid_split * dataset_size)\n",
    "# 计算训练集的大小\n",
    "train_size = dataset_size - valid_size\n",
    "# 利用 random_split 函数随机划分数据集\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size])\n",
    "\n",
    "# 创建 DataLoader 对象\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "print(len(train_loader))\n",
    "print(len(test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 創立model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CNN Model\n",
    "class CNN_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_Model, self).__init__()\n",
    "        # Convolution 1 , input_shape=(3,128,480)\n",
    "        self.cnn1 = nn.Conv2d(in_channels=3, out_channels=25, kernel_size=3, stride=1, padding=0) #output_shape=(25,126,478)\n",
    "        self.relu1 = nn.ReLU() # activation\n",
    "        # Max pool 1\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2) #output_shape=(25,63,239)\n",
    "        # Convolution 2\n",
    "        self.cnn2 = nn.Conv2d(in_channels=25, out_channels=32, kernel_size=6, stride=1, padding=0) #output_shape=(32,58,234)\n",
    "        self.relu2 = nn.ReLU() # activation\n",
    "        # Avg pool 1\n",
    "        self.avgpool1 = nn.AvgPool2d(kernel_size=2) #output_shape=(32,29,117)\n",
    "        # Convolution 3\n",
    "        self.cnn3 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=4, stride=1, padding=0) #output_shape=(32,26,114)\n",
    "        self.relu3 = nn.ReLU() # activation\n",
    "        # Max pool 2\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2) #output_shape=(32,13,57)\n",
    "        # Fully connected 1 ,#input_shape=(32*13*57)\n",
    "        self.fc1 = nn.Linear(32 * 13 * 57, 1024) \n",
    "        # Fully connected 2\n",
    "        self.fc2 = nn.Linear(1024, 64)\n",
    "        # Fully connected 3\n",
    "        self.fc3 = nn.Linear(64, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Convolution 1\n",
    "        out = self.cnn1(x)\n",
    "        out = self.relu1(out)\n",
    "        # Max pool 1\n",
    "        out = self.maxpool1(out)\n",
    "        # Convolution 2 \n",
    "        out = self.cnn2(out)\n",
    "        out = self.relu2(out)\n",
    "        # Avg pool 1\n",
    "        out = self.avgpool1(out)\n",
    "        # Convolution 3\n",
    "        out = self.cnn3(out)\n",
    "        out = self.relu3(out)\n",
    "        # Max pool 2 \n",
    "        out = self.maxpool2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        # Linear function (readout)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's structure:\n",
      " =================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "CNN_Model                                --\n",
      "├─Conv2d: 1-1                            700\n",
      "├─ReLU: 1-2                              --\n",
      "├─MaxPool2d: 1-3                         --\n",
      "├─Conv2d: 1-4                            28,832\n",
      "├─ReLU: 1-5                              --\n",
      "├─AvgPool2d: 1-6                         --\n",
      "├─Conv2d: 1-7                            16,416\n",
      "├─ReLU: 1-8                              --\n",
      "├─MaxPool2d: 1-9                         --\n",
      "├─Linear: 1-10                           24,282,112\n",
      "├─Linear: 1-11                           65,600\n",
      "├─Linear: 1-12                           130\n",
      "=================================================================\n",
      "Total params: 24,393,790\n",
      "Trainable params: 24,393,790\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "import torchinfo\n",
    "import torchinfo\n",
    "\n",
    "model = CNN_Model()\n",
    "print(f\"Model's structure:\\n {torchinfo.summary(model)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 訓練模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1/20, Time:18.45 seconds, Train Loss: 0.8888\n",
      "Epoch:2/20, Time:18.04 seconds, Train Loss: 0.6686\n",
      "Epoch:3/20, Time:17.23 seconds, Train Loss: 0.6333\n",
      "Epoch:4/20, Time:17.35 seconds, Train Loss: 0.4957\n",
      "Epoch:5/20, Time:39.11 seconds, Train acc:0.9811643835616438, Train Loss: 0.2327, Valid acc:0.9948453608247423\n",
      "Epoch:6/20, Time:17.39 seconds, Train Loss: 0.0638\n",
      "Epoch:7/20, Time:17.60 seconds, Train Loss: 0.0644\n",
      "Epoch:8/20, Time:17.04 seconds, Train Loss: 0.3024\n",
      "Epoch:9/20, Time:17.26 seconds, Train Loss: 0.0391\n",
      "Epoch:10/20, Time:39.43 seconds, Train acc:0.9982876712328768, Train Loss: 0.0350, Valid acc:0.979381443298969\n",
      "Epoch:11/20, Time:17.11 seconds, Train Loss: 0.0127\n",
      "Epoch:12/20, Time:17.64 seconds, Train Loss: 0.0081\n",
      "Epoch:13/20, Time:17.47 seconds, Train Loss: 0.0042\n",
      "Epoch:14/20, Time:17.13 seconds, Train Loss: 0.0040\n",
      "Epoch:15/20, Time:40.94 seconds, Train acc:1.0, Train Loss: 0.0029, Valid acc:0.9896907216494846\n",
      "Epoch:16/20, Time:17.87 seconds, Train Loss: 0.0015\n",
      "Epoch:17/20, Time:17.64 seconds, Train Loss: 0.0015\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     16\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m, (\u001b[38;5;28minput\u001b[39m, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m     18\u001b[0m   \u001b[38;5;66;03m# Move data to \"device\".\u001b[39;00m\n\u001b[0;32m     19\u001b[0m   \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     20\u001b[0m   target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32md:\\ancode\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\ancode\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\ancode\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32md:\\ancode\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataset.py:419\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[1;32md:\\ancode\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataset.py:419\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[1;32md:\\ancode\\envs\\pytorch\\lib\\site-packages\\torchvision\\datasets\\folder.py:229\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;124;03m    index (int): Index\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;124;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    228\u001b[0m path, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[index]\n\u001b[1;32m--> 229\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    231\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(sample)\n",
      "File \u001b[1;32md:\\ancode\\envs\\pytorch\\lib\\site-packages\\torchvision\\datasets\\folder.py:268\u001b[0m, in \u001b[0;36mdefault_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpil_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\ancode\\envs\\pytorch\\lib\\site-packages\\torchvision\\datasets\\folder.py:248\u001b[0m, in \u001b[0;36mpil_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    247\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(f)\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\ancode\\envs\\pytorch\\lib\\site-packages\\PIL\\Image.py:922\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\n\u001b[0;32m    875\u001b[0m     \u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mPalette\u001b[38;5;241m.\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m\n\u001b[0;32m    876\u001b[0m ):\n\u001b[0;32m    877\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[0;32m    879\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    919\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[0;32m    920\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 922\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    924\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[0;32m    925\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    926\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32md:\\ancode\\envs\\pytorch\\lib\\site-packages\\PIL\\ImageFile.py:291\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    288\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m    290\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 291\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "# Move model to \"device\".\n",
    "model = CNN_Model().to(device)\n",
    "# File name.\n",
    "modelSaveName = \"MyModel.pt\"\n",
    "# Create an optimizer.\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=learning_rate)\n",
    "# Create a loss function\n",
    "lossFunc = nn.CrossEntropyLoss()\n",
    "\n",
    "bestValidAcc = 0\n",
    "for epoch in range(1, num_epochs+1):\n",
    "  start_time = time.time()  # 計算每個epoch開始的時間\n",
    "  # Training mode.\n",
    "  model.train()\n",
    "  train_loss = 0.0\n",
    "  for iter, (input, target) in enumerate(train_loader):\n",
    "    # Move data to \"device\".\n",
    "    input = input.to(device)\n",
    "    target = target.to(device)\n",
    "    # Forward pass.\n",
    "    output = model(input)\n",
    "    # Compute the loss.\n",
    "    loss=lossFunc(output, target)\n",
    "    # Clear optimizer gradients.\n",
    "    optimizer.zero_grad()\n",
    "    # Loss backward propagation.\n",
    "    loss.backward()\n",
    "    # Update all learnable parameters.\n",
    "    optimizer.step()\n",
    "    # update training loss\n",
    "    train_loss += loss.item()*input.size(0)\n",
    "\n",
    "  if epoch % 5 == 0: \n",
    "    # Evaluation mode.\n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "      correctCount=0\n",
    "      for input, target in train_loader:\n",
    "        input, target = input.to(device), target.to(device)\n",
    "        output = model(input).max(1)[1]\n",
    "        correctCount+=torch.sum(output==target).item()\n",
    "        trainAcc=correctCount/len(train_loader.dataset)\n",
    "\n",
    "      correctCount=0\n",
    "      for input, target in test_loader:\n",
    "        input, target = input.to(device), target.to(device)\n",
    "        output = model(input).max(1)[1]\n",
    "        correctCount+=torch.sum(output==target).item()\n",
    "        validAcc=correctCount/len(test_loader.dataset)\n",
    "\n",
    "      # 計算每個樣本的平均損失\n",
    "      train_loss = train_loss / len(train_loader.dataset)\n",
    "\n",
    "    end_time = time.time()  # 計算每個epoch結束的時間\n",
    "    epoch_time = end_time - start_time  # 計算每個epoch所花的時間\n",
    "    print(\"Epoch:{}/{}, Time:{:.2f} seconds, Train acc:{}, Train Loss: {:.4f}, Valid acc:{}\".format(epoch, num_epochs, epoch_time, trainAcc, train_loss, validAcc))\n",
    "    if validAcc>bestValidAcc:\n",
    "      bestValidAcc=validAcc\n",
    "      torch.save(model, modelSaveName)\n",
    "\n",
    "  else:\n",
    "    # 計算每個樣本的平均損失\n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    end_time = time.time()  # 計算每個epoch結束的時間\n",
    "    epoch_time = end_time - start_time  # 計算每個epoch所花的時間\n",
    "    print(\"Epoch:{}/{}, Time:{:.2f} seconds, Train Loss: {:.4f}\".format(epoch, num_epochs, epoch_time, train_loss))\n",
    "\n",
    "print(\"best valid acc:\",bestValidAcc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
